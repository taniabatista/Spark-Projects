{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEC 3: Trending Topics\n",
    "\n",
    "## 1. Trending Topics Básico\n",
    "\n",
    "- Use Structured Streaming to implement a simple version of \"trending topics\".\n",
    "\n",
    "- CSV files in `data/tweets`. Each CSV line corresponds to a tweet.\n",
    "\n",
    "- Do a Spark Structured Streaming 'job' that will process the files one by one, and maintain a table in memory with the 20 most commented terms.\n",
    "\n",
    "- We will consider the tweets that come in the correct order, ie, its not necessary to use data 'timestamp'\n",
    "\n",
    "- Solve this exercise using Structured Streaming, not a static Spark job.\n",
    "\n",
    "- The result of the calculations on the stream should store them in a memory table called `trending`.\n",
    "\n",
    "- The following query should show the final result. It will change with time, as new files are processed.\n",
    "```python\n",
    "spark.sql(\"select * from trending\").show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I tried really hard to make it NOT static, like in the instructions\n",
    "# I followed a step-by-step guide to have this running: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n",
    "# Did not work at ALL. I had to make a static DF.\n",
    "\n",
    "# nc -lk 9999\n",
    "# lines = spark \\\n",
    "#     .readStream \\\n",
    "#     .format(\"socket\") \\\n",
    "#     .option(\"host\", \"localhost\") \\\n",
    "#     .option(\"port\", 9999) \\\n",
    "#     .load()\n",
    "# lines.printSchema()\n",
    "\n",
    "# SPARK.READ the static stream\n",
    "    # ex static = spark.read.json(\"data/tweets/\") --> CSV\n",
    "static = spark.read.option(\"header\",\"true\").option(\"sep\",\";\").csv(\"data/tweets/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- username: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- retweets: string (nullable = true)\n",
      " |-- favorites: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- mentions: string (nullable = true)\n",
      " |-- hashtags: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- permalink: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print static Schema\n",
    "static.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(username='librarian2277', date='2017-10-14 00:54', retweets='0', favorites='0', text='How to write blog headlines that drive search traffic https:// searchenginewatch.com/2017/10/07/how -to-write-blog-headlines-that-drive-search-traffic/ …', geo=None, mentions=None, hashtags=None, id='919063962876104704', permalink='https://twitter.com/librarian2277/status/919063962876104704')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stream the data\n",
    "    # `readStream` =  monitor tweets directory\n",
    "    # `maxFilesPerTrigger`: flow the new files 1 by 1.\n",
    "    # streaming = spark.readStream.schema(static.schema).option(\"maxFilesPerTrigger\", 1).json(\"data/activity-data\")\n",
    "\n",
    "streaming = spark.readStream.option(\"sep\",\";\")\\\n",
    "    .option(\"maxFilesPerTrigger\", 10)\\\n",
    "    .schema(static.schema)\\\n",
    "    .csv(\"data/tweets\")\n",
    "\n",
    "# Confirm that it is indeed streaming\n",
    "streaming.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- username: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- retweets: string (nullable = true)\n",
      " |-- favorites: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- mentions: string (nullable = true)\n",
      " |-- hashtags: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- permalink: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT /separate the words\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# EXPLODE each word into a new DataFrame row\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# how to do it all in one line: spark.read.csv(\"data/twitterBios\").select(explode(split(\"_c0\", \"\\s+\")).alias(\"word\")).groupBy(\"word\").count().orderBy(\"count\", ascending=False).limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- username: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- retweets: string (nullable = true)\n",
      " |-- favorites: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- mentions: string (nullable = true)\n",
      " |-- hashtags: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- permalink: string (nullable = true)\n",
      " |-- split: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- word: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ex: words = streaming.select(explode(split(lines.value, \" \")).alias(\"word\"))\n",
    "words = streaming.withColumn(\"split\", split(col(\"text\"), \" \")).withColumn(\"word\", explode(col(\"split\")))\n",
    "words.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# WORDCOUNT: groupBy words, then count groups\n",
    "\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "wordCounts.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STREAMQUERY: \n",
    "# ex: activityQuery = activityCounts.writeStream.queryName(\"activity_counts\").format(\"memory\").outputMode(\"complete\").start()\n",
    "    # indicate OUTPUTMODE: complete\n",
    "    # indicate output FORMAT: memory\n",
    "    # NAME it with a unique number, for consulting later. streaming_wordcount --> trending\n",
    "    # START streaming process\n",
    "\n",
    "query = wordCounts\\\n",
    "    .writeStream\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .queryName(\"trending\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.StreamingQuery at 0x7fd9eaa7ab00>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ACTIVE: Confirm that the streaming is active\n",
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n",
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n",
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n",
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n",
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query over results in memory\n",
    "for x in range(5):\n",
    "    spark.sql(\"SELECT * FROM trending\").show()\n",
    "    \n",
    "# Unfortunately this never worked:\n",
    "# spark.sql(\"select * from trending\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to STOP the streaming\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Eliminando \"stopwords\"\n",
    "\n",
    "\n",
    "- Find the trending topics again. Avoid \"stopwords\", which can be found in folder `data/stopwords`\n",
    "- Stopwords are common words that don't mean very much\n",
    "\n",
    "- You have to do this calculation over the flow of tweets. Again you must process the files one by one. \n",
    "- But this time you should avoid for the stopwords to appear as trending topics. \n",
    "- Keep in mind that no combination of capitalization should appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Spark Joins, we need a static DF.\n",
    "#  Read the data as a dataframe\n",
    "myStaticDF = spark.read.csv(\"data/stopwords/stopwords.txt\").toDF(\"stopWords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|  stopWords|\n",
      "+-----------+\n",
      "|          a|\n",
      "|       able|\n",
      "|      about|\n",
      "|      above|\n",
      "|  according|\n",
      "|accordingly|\n",
      "|     across|\n",
      "|   actually|\n",
      "|      after|\n",
      "| afterwards|\n",
      "|      again|\n",
      "|    against|\n",
      "|        all|\n",
      "|      allow|\n",
      "|     allows|\n",
      "|     almost|\n",
      "|      alone|\n",
      "|      along|\n",
      "|    already|\n",
      "|       also|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myStaticDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTEMPT TO JOIN THE DATAFRAMES:\n",
    "    # add a 3rd column, \"stopWords\". Will contain only stop-words and nulls.\n",
    "    # if value in the new 3rd column = empty, then this means it was never a stop word\n",
    "    # if value is NOT empty, then it is a stop word. Drop it!\n",
    "    \n",
    "streamingJoin = wordCounts\\\n",
    "    .join(myStaticDF, col(\"word\") == col(\"stopWords\"),\"left_outer\")\\\n",
    "    .where(\"stopWords is null\")\\\n",
    "    .drop(\"stopWords\")\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"stopWords_join\").format(\"memory\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()\n",
    "\n",
    "#spark.sql(\"select * from stopWords_join\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Evitar signos de puntuación\n",
    "\n",
    "\n",
    "- We have eliminated the stop words\n",
    "- For our 'trending topics' we want to prevent the appearance of words that are solely punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGEX PATTERNS\n",
    "    # ^ match must start at the beginning of the string or line\n",
    "    # \\A  match must occur at the start of the string\n",
    "    # \\p{ name } Matches any single character that IS Unicode general\n",
    "    # \\P{ name } Matches any single character that is NOT  Unicode \n",
    "    # \\p{Punct} Punctuation: 1 of !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "\n",
    "# How to match a word that contains no letters, only punctuation\n",
    "punctuationWord = \"[^\\p{Punct}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the previous dataframe to drop punctuationWord\n",
    "\n",
    "noPunctuationWords = streamingJoin\\\n",
    "    .where(col(\"word\")\\\n",
    "    .rlike(punctuationWord))\\\n",
    "    .drop(\"word\")\\\n",
    "    .writeStream.queryName(\"punctuation_join\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start at another failed attempt...\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "streamingJoin.where(value == punctuationWord).drop(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
